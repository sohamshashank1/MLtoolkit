{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TF_Exercise_Text.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/sohamshashank1/MLtoolkit/blob/master/TF_Exercise_Text.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "CgT6L3lEtaEb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "***BAG OF WORDS***"
      ]
    },
    {
      "metadata": {
        "id": "X9w9g53otD0T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import numpy as np\n",
        "import csv\n",
        "import string\n",
        "import requests\n",
        "import io\n",
        "from zipfile import ZipFile\n",
        "from tensorflow.contrib import learn\n",
        "from tensorflow.python.framework import ops\n",
        "ops.reset_default_graph()\n",
        "\n",
        "# Start a graph session\n",
        "sess = tf.Session()\n",
        "\n",
        "# Check if data was downloaded, otherwise download it and save for future use\n",
        "save_file_name = os.path.join('temp','temp_spam_data.csv')\n",
        "\n",
        "# Create directory if it doesn't exist\n",
        "if not os.path.exists('temp'):\n",
        "    os.makedirs('temp')\n",
        "\n",
        "if os.path.isfile(save_file_name):\n",
        "    text_data = []\n",
        "    with open(save_file_name, 'r') as temp_output_file:\n",
        "        reader = csv.reader(temp_output_file)\n",
        "        for row in reader:\n",
        "            text_data.append(row)\n",
        "else:\n",
        "    zip_url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip'\n",
        "    r = requests.get(zip_url)\n",
        "    z = ZipFile(io.BytesIO(r.content))\n",
        "    file = z.read('SMSSpamCollection')\n",
        "    # Format Data\n",
        "    text_data = file.decode()\n",
        "    text_data = text_data.encode('ascii',errors='ignore')\n",
        "    text_data = text_data.decode().split('\\n')\n",
        "    text_data = [x.split('\\t') for x in text_data if len(x)>=1]\n",
        "    \n",
        "    # And write to csv\n",
        "    with open(save_file_name, 'w') as temp_output_file:\n",
        "        writer = csv.writer(temp_output_file)\n",
        "        writer.writerows(text_data)\n",
        "\n",
        "texts = [x[1] for x in text_data]\n",
        "target = [x[0] for x in text_data]\n",
        "\n",
        "# Relabel 'spam' as 1, 'ham' as 0\n",
        "target = [1 if x == 'spam' else 0 for x in target]\n",
        "texts = [x.lower() for x in texts]\n",
        "\n",
        "# Remove punctuation\n",
        "texts = [''.join(c for c in x if c not in string.punctuation) for x in texts]\n",
        "\n",
        "# Remove numbers\n",
        "texts = [''.join(c for c in x if c not in '0123456789') for x in texts]\n",
        "\n",
        "# Trim extra whitespace\n",
        "texts = [' '.join(x.split()) for x in texts]\n",
        "\n",
        "sentence_size = 25\n",
        "min_word_freq = 3\n",
        "\n",
        "# Setup vocabulary processor\n",
        "vocab_processor = learn.preprocessing.VocabularyProcessor(sentence_size, min_frequency=min_word_freq)\n",
        "\n",
        "# Have to fit transform to get length of unique words.\n",
        "vocab_processor.transform(texts)\n",
        "transformed_texts = np.array([x for x in vocab_processor.transform(texts)])\n",
        "embedding_size = len(np.unique(transformed_texts))\n",
        "\n",
        "# Split up data set into train/test\n",
        "train_indices = np.random.choice(len(texts), round(len(texts)*0.8), replace=False)\n",
        "test_indices = np.array(list(set(range(len(texts))) - set(train_indices)))\n",
        "texts_train = [x for ix, x in enumerate(texts) if ix in train_indices]\n",
        "texts_test = [x for ix, x in enumerate(texts) if ix in test_indices]\n",
        "target_train = [x for ix, x in enumerate(target) if ix in train_indices]\n",
        "target_test = [x for ix, x in enumerate(target) if ix in test_indices]\n",
        "\n",
        "# Setup Index Matrix for one-hot-encoding\n",
        "identity_mat = tf.diag(tf.ones(shape=[embedding_size]))\n",
        "\n",
        "# Create variables for logistic regression\n",
        "A = tf.Variable(tf.random_normal(shape=[embedding_size,1]))\n",
        "b = tf.Variable(tf.random_normal(shape=[1,1]))\n",
        "\n",
        "# Initialize placeholders\n",
        "x_data = tf.placeholder(shape=[sentence_size], dtype=tf.int32)\n",
        "y_target = tf.placeholder(shape=[1, 1], dtype=tf.float32)\n",
        "\n",
        "# Text-Vocab Embedding\n",
        "x_embed = tf.nn.embedding_lookup(identity_mat, x_data)\n",
        "x_col_sums = tf.reduce_sum(x_embed, 0)\n",
        "\n",
        "# Declare model operations\n",
        "x_col_sums_2D = tf.expand_dims(x_col_sums, 0)\n",
        "model_output = tf.add(tf.matmul(x_col_sums_2D, A), b)\n",
        "\n",
        "# Declare loss function (Cross Entropy loss)\n",
        "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=model_output, labels=y_target))\n",
        "\n",
        "# Prediction operation\n",
        "prediction = tf.sigmoid(model_output)\n",
        "\n",
        "# Declare optimizer\n",
        "my_opt = tf.train.GradientDescentOptimizer(0.001)\n",
        "train_step = my_opt.minimize(loss)\n",
        "\n",
        "# Intitialize Variables\n",
        "init = tf.global_variables_initializer()\n",
        "sess.run(init)\n",
        "\n",
        "# Start Logistic Regression\n",
        "print('Starting Training Over {} Sentences.'.format(len(texts_train)))\n",
        "loss_vec = []\n",
        "train_acc_all = []\n",
        "train_acc_avg = []\n",
        "for ix, t in enumerate(vocab_processor.fit_transform(texts_train)):\n",
        "    y_data = [[target_train[ix]]]\n",
        "    \n",
        "    # Run through each observation for training\n",
        "    sess.run(train_step, feed_dict={x_data: t, y_target: y_data})\n",
        "    temp_loss = sess.run(loss, feed_dict={x_data: t, y_target: y_data})\n",
        "    loss_vec.append(temp_loss)\n",
        "    \n",
        "    if (ix + 1) % 10 == 0:\n",
        "        print('Training Observation #{}, Loss = {}'.format(ix+1, temp_loss))\n",
        "        \n",
        "    # Keep trailing average of past 50 observations accuracy\n",
        "    # Get prediction of single observation\n",
        "    [[temp_pred]] = sess.run(prediction, feed_dict={x_data:t, y_target:y_data})\n",
        "    # Get True/False if prediction is accurate\n",
        "    train_acc_temp = target_train[ix]==np.round(temp_pred)\n",
        "    train_acc_all.append(train_acc_temp)\n",
        "    if len(train_acc_all) >= 50:\n",
        "        train_acc_avg.append(np.mean(train_acc_all[-50:]))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jdUAKnAPtZzL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}