# ************ Import all the necessary packages ************

import re
import pandas as pd
import networkx as nx
import numpy as np
import difflib
from metaphone import doublemetaphone
from difflib import SequenceMatcher
import os
import teradata
import time
import jellyfish as jl

# Change the director
os.chdir('/storage/EIX_LAB_NH_1/msk/Python/CFOUR')

# Create empty dataframes and network graph to append results later

G=nx.Graph()
#fc = pd.DataFrame()
#fcc = pd.DataFrame()
#clus = pd.DataFrame()
#cluss = pd.DataFrame()

# Import csv table to pandas data frame - Single Column and do all the necessary cleanings

df = pd.read_csv('merchname.csv', sep=',',  encoding='latin-1')
df.columns = ['bnm']
df.dropna(axis=0, how='any', thresh=None, subset=None, inplace=True)
df.drop_duplicates(keep = 'last',inplace = True)
df['cbsnm'] = df['bnm'].str.replace('[^a-zA-Z\s]','')
df['cbsnm'] = df['cbsnm'].apply(lambda x: x.strip())
df['cbsnm'] = df['cbsnm'].apply(lambda x: re.sub(r'\s+', ' ', x))
df['cbsnm'] = df['cbsnm'].apply(lambda x: x.upper())

# Get the double metaphone and parse the first 4 characters to cluster the cleaned business names based on the double metaphone

cbn = pd.DataFrame(df.ix[:,1])
cbn['cbsnm'] = cbn['cbsnm'].apply(lambda x: x.upper())
cbn.drop_duplicates(keep = 'last',inplace = True)
cbn['metap'] = cbn['cbsnm'].apply(lambda x: doublemetaphone(x)[0])
cbn['meta'] = cbn['metap'].apply(lambda x: x[:4])
cbn['id'] = pd.Series(cbn['meta']).astype('category').cat.codes
cbn.sort_values(by=['id'],ascending=[True],inplace = True)
cbn['id'] = cbn['id'].apply(lambda x: x+1)

# Get the cluster counts to pass it to the Jaccard step

cnt = cbn.groupby('id').count().reset_index()
cnt.drop(cnt.columns[[2, 3]], axis=1, inplace=True)
cnt.columns=['id','Count']
counts = cnt.loc[(cnt.Count > 1) & (cnt.Count <= 3000)]
countssi = cnt.loc[cnt.Count == 1]
countscu = cnt.loc[cnt.Count > 3000]
#counts.sort_values(by=['Count'],ascending=[True],inplace = True)

# Get those clusters where count(*) is > 3000 and re-cluster using the entire double metaphone value
fl = cbn.loc[cbn['id'].isin(countscu['id'])]
fl.drop(fl.columns[[2, 3]], axis=1, inplace=True)
fl['id'] = pd.Series(fl['metap']).astype('category').cat.codes
fl.sort_values(by=['id'],ascending=[True],inplace = True)
fl['id'] = fl['id'].apply(lambda x: x+1)

# Get the cluster counts to pass it to the Jaccard step

nct = fl.groupby('id').count().reset_index()
nct.drop(nct.columns[[2]], axis=1, inplace=True)
nct.columns=['id','Count']
ncounts = nct.loc[(nct.Count > 1) & (nct.Count <= 3000)]
ncountssi = nct.loc[nct.Count == 1]
ncountscu = nct.loc[nct.Count > 3000]

countss = nct.loc[(nct.Count > 1) & (nct.Count <= 3000)]
#countss.sort_values(by=['Count'],ascending=[True],inplace = True)

counts = counts.append(countss)

# Function for getting the Jaccard dissimilarity score using n-grams, change accordingly

def jaccard(a,b,n):
 s = [a[i:i+n] for i in range(len(a)-n+1)]
 t = [b[i:i+n] for i in range(len(b)-n+1)]
 if len(list(set(s) | set(t))) >0:
  jac_coeff = 1 -  len(list(set(s) & set(t))) / len(list(set(s) | set(t)))
 else:
  jac_coeff = 1
 return jac_coeff

# Function to obtain all the combinations with Jaccard score less than a cut-off value

def jaccomb(x):
 ds = pd.DataFrame(cbn.loc[cbn['id'] == counts['id'].iloc[x]].ix[:,0])
 sj = pd.DataFrame([ (i,j) for i in ds.cbsnm for j in ds.cbsnm if (i>j) ])
 sj['sim'] = sj.apply(lambda z: jaccard(z[0], z[1],2), axis=1)
 tj = pd.DataFrame(sj.loc[sj['sim']<=0.5].ix[:,0:3])
 tj.columns = ['nm1','nm2','jac']
 return(tj)

fc = pd.concat(map(jaccomb, range(0,len(counts))))

#fcc = pd.concat(map(jaccomb, range(0,len(countss))))

# Save the combinations that satisfied a particular cut-off for future use

#fc.to_pickle('jaccard.pkl')

# Obtain all the combinations with Jaccard score less than a cut-off value (Repeated step)

# Append the obtained dataframe to that obtained in the previous jaccard step

#fc = fc.append(fcc)

fc.to_pickle('jaccard06.pkl')

# Intutively select a cutoff and get those combinations for the optnet step

# Cluster the connected components to get the final clustering

def finonet(cutoff):
 fd = fc[fc.jac <= cutoff][['nm1','nm2']]
 fd.columns = ['from','to']
 G = nx.from_pandas_dataframe(fd, 'from' , 'to', edge_attr=None, create_using=None)
 c = sorted(nx.connected_components(G), key=len, reverse=True)
 def optnet(i):
  s = pd.DataFrame(list(c[i]))
  s['id'] = i +1
  return(s)
 clus = pd.concat(map(optnet, range(0,len(c))))
 clus.columns = ['bnm','id']
 singl = cbn.loc[~cbn['cbsnm'].isin(clus['bnm'])]
 singl.drop(singl.columns[[1,2,3]], axis=1, inplace=True)
 singl.columns = ['bnm']
 singl['id'] = 0
 fclus = clus.append(singl)
 fclus.columns = ['cbsnm','id']
 return(fclus)

fulclus01 = finonet(0.1)

# Join the master set on cleaned business name to get the non-cleaned clustering and save as a csv for further processing

master = pd.merge(df,fulclus01,on='cbsnm')
master.to_csv('finclus2.csv', encoding='utf-8', index = False)


***************************************************  End of Code ***********************************************************




































***************************************************  All Trials Here ***********************************************************

#udaExec = teradata.UdaExec (appName="HelloWorld", version="1.0",logConsole=False)
#conn = udaExec.connect("wapr")
#query = "create table user_ix.msk_final as select * from master"

#with udaExec.connect("wapr") as session:
 
#master.to_sql('user_ix.msk_final', conn, flavor='sqlite',if_exists='fail', index=True, index_label=None, chunksize=50000, dtype=None)

#df.to_csv('cluss.csv', sep='\t', encoding='utf-8', index = False)

#clus.to_sas('cluss.sas7bdat', sep='\t', encoding='utf-8', index = False)

fc = pd.read_pickle('jaccard2.pkl')

for i in range(0, len(c)):
    l = pd.DataFrame(list(zip(list(c[i]),[i+1]*len(list(c[i])))))
    l.columns = ['bnm','id']
    clus = clus.append(l)
    del l

start = time.clock()
test = pd.concat(map(optnet, range(0,len(c)))) 
print time.clock() - start

for x in range(0, len(counts1)):
    ds = pd.DataFrame(cbn.loc[cbn['id'] == counts['id'].iloc[x]].ix[:,0])
    sj = pd.DataFrame([ (i,j) for i in ds.cbsnm for j in ds.cbsnm if (i>j) ])
    sj['sim'] = sj.apply(lambda z: jaccard(z[0], z[1],2), axis=1)
    tj = pd.DataFrame(sj.loc[sj['sim']<=0.6].ix[:,0:3])
    tj.columns = ['nm1','nm2','jac']
    fc = fc.append(tj)
    del ds,sj,tj
    print(x)

for x in range(0, len(countss)):
    ds = pd.DataFrame(fl.loc[fl['id'] == countss['id'].iloc[x]].ix[:,0])
    sj = pd.DataFrame([ (i,j) for i in ds.cbsnm for j in ds.cbsnm if (i>j) ])
    sj['sim'] = sj.apply(lambda z: jaccard(z[0], z[1],2), axis=1)
    tj = pd.DataFrame(sj.loc[sj['sim']<=0.6].ix[:,0:3])
    tj.columns = ['nm1','nm2','jac']
    fcc = fcc.append(tj)
    del ds,sj,tj
    print(x)

fd = fc[fc.jac <= 0.4][['nm1','nm2']]
fd.columns = ['from','to']

# Create a network graph using the combinations as connected components

G = nx.from_pandas_dataframe(fd, 'from' , 'to', edge_attr=None, create_using=None)
c = sorted(nx.connected_components(G), key=len, reverse=True)

clus = pd.concat(map(optnet, range(0,len(c)))) 

clus.to_pickle('clust.pkl')

# Get those business names which are not present in the obtained clusters and give an id of zero (0)
singl = cbn.loc[~cbn['cbsnm'].isin(clus['bnm'])]
singl.drop(singl.columns[[1,2,3]], axis=1, inplace=True)
singl.columns = ['bnm']
singl['id'] = 0

# Append the clusetered business names with those who were assigned an id of zero to get the master set

fclus = clus.append(singl)
fclus.columns = ['cbsnm','id']













































#export http_proxy=http://zkw8u7b:password@webproxy.bankofamerica.com:8080
#export https_proxy=https://zkw8u7b:password@webproxy.bankofamerica.com:8080

#os.getcwd()
#^a-zA-Z\d\s: Regex for alpha,numbers and white spaces
#difflib.SequenceMatcher(None, "CHRISTOPHER A BURTON", "CHRISTOPHER A NELSON").ratio()

# Import all the necessary modules
#import teradata
#import multiprocessing
#import nx
#from fuzzywuzzy import fuzz
#from fuzzywuzzy import process
#from itertools import combinations
#from joblib import Parallel, delayed

#os.chdir('/storage/EIX_LAB_NH_1/msk/Python/All/Bolt')
#os.chdir('/storage/EIX_LAB_NH_1/msk/Python/All/Bolt_35m')


#jacs = pd.read_pickle('fcboltpy.pkl')

#ds = pd.read_csv('bolt.csv', sep=',',  encoding='latin-1')

#d1 = pd.DataFrame(ds.ix[:,0])

#d2 = pd.DataFrame(ds.ix[:,1])

#d1.columns = ['bnm']

#d2.columns = ['bnm']

#df = pd.concat([d1, d2], ignore_index=True)

#df.sort_values(by=['bnm'],ascending=[True],inplace = True)

#cbn.dropna(axis=0, how='any', thresh=None, subset=None, inplace=True)

#24270

# Using Sequence Matcher

for x in range(0, len(counts)):
    ds = pd.DataFrame(cbn.loc[cbn['id'] == counts['id'].iloc[x]].ix[:,0])
    sj = pd.DataFrame([ (i,j) for i in ds.cbsnm for j in ds.cbsnm if (i>j) ])
    sj['sim'] = sj.apply(lambda z: SequenceMatcher(None, z[0], z[1]).ratio(), axis=1)
    tj = pd.DataFrame(sj.loc[sj['sim']>=0.5].ix[:,0:3])
    tj.columns = ['nm1','nm2','jac']
    fc = fc.append(tj)
    del ds,sj,tj
    print(x)

#fc.to_pickle('boltsim.pkl')

#fc = pd.read_pickle('boltsim.pkl')

fd = fc[fc.jac <= 0.4][['nm1','nm2']]

fd.columns = ['from','to']

# Syntax: from_pandas_dataframe(df, source, target, edge_attr=None, create_using=None)

G = nx.from_pandas_dataframe(fd, 'from' , 'to', edge_attr=None, create_using=None)

c = sorted(nx.connected_components(G), key=len, reverse=True)

for i in range(0, len(c)):
    l = pd.DataFrame(list(c[i]))
    l['id'] = i +1
    l.columns = ['bnm','id']
    cluss = cluss.append(l)
    del l
    print(i)

clus.to_pickle('clust02.pkl')

fl = cbn.loc[~cbn['id'].isin(clus['id'])]



































fc.to_pickle('bolt35fuz.pkl')
fc.to_pickle('bolt35jac.pkl')

import time

for i in range(0, 6):
 print(i)
 time.sleep(4)
time.sleep(5)

l.sort_values(by=['bnm'],ascending=[True],inplace = True)
l['id'] = i +1

fc.to_pickle('boltjac500.pkl')
fc.to_pickle('boltjac3000.pkl')

result = pd.merge(left, right, on='k')
len(results['Primary_Name'].unique())

for x in range(0, len(counts)):
    ds = pd.DataFrame(new.loc[new['id'] == counts['id'].iloc[x]].ix[:,0])
    sj = pd.DataFrame([ (i,j) for i in ds.bnm for j in ds.bnm if (i>j) ])
    sj['sim'] = sj.apply(lambda z: jaccard(z[0], z[1],2), axis=1)
    tj = pd.DataFrame(sj.loc[sj['sim']<=0.6].ix[:,0:3])
    tj.columns = ['nm1','nm2','jac']
    fc = fc.append(tj)
    del ds,sj,tj
    print(x)
