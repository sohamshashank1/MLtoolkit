{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TF_Exercise1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/sohamshashank1/MLtoolkit/blob/master/TF_Exercise1.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "3JThqx6AOkNX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "BASIC OPERATIONS"
      ]
    },
    {
      "metadata": {
        "id": "VobaVp6iax0D",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "const = tf.constant(2.0, name = \"const\")\n",
        "b = tf.Variable(2.0, name = 'b')\n",
        "b\n",
        "c = tf.Variable(3.0, name = 'c')\n",
        "d = tf.multiply(b,c)\n",
        "e = tf.add(c, const, name='e')\n",
        "a = tf.multiply(d, e, name='a')\n",
        "\n",
        "init_op = tf.global_variables_initializer()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  sess.run(init_op)\n",
        "  a_out = sess.run(a)\n",
        "  print(\"variable a is {}\".format(a_out))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fwi8M6XCrezh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "my_var = tf.Variable(tf.ones([2,3]))\n",
        "sess = tf.Session()\n",
        "initialize_op = tf.global_variables_initializer ()\n",
        "sess.run(initialize_op)\n",
        "out1 = sess.run(my_var)\n",
        "out1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9zHSCfCZs95B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sess = tf.Session()\n",
        "x = tf.placeholder(tf.float32, shape=[2,2])\n",
        "y = tf.identity(x)\n",
        "x_vals = np.random.rand(2,2)\n",
        "sess.run(y, feed_dict={x: x_vals})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zCKjkUUZtXF9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Assigning session to a variable \n",
        "# creating a graph session by running the following code:\n",
        "sess = tf.Session()\n",
        "\n",
        "first_var = tf.Variable(tf.zeros([2,4]))\n",
        "sess.run(first_var.initializer)\n",
        "second_var = tf.Variable(tf.zeros_like(first_var))\n",
        "# Depends on first_var\n",
        "sess.run(second_var.initializer)\n",
        "sess.run(second_var)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PISrkj1KtWXw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "identity_matrix = tf.diag([1.0, 1.0, 1.0])\n",
        "A = tf.truncated_normal([2, 3])\n",
        "B = tf.fill([2,3], 5.0)\n",
        "C = tf.random_uniform([3,2])\n",
        "D = tf.convert_to_tensor(np.array([[1., 2., 3.],[-3., -7., -1.],[0., 5., -2.]]))\n",
        "print(sess.run(identity_matrix))\n",
        "print(sess.run(D))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uF53TzuqyI8i",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(sess.run(tf.matmul(B, identity_matrix)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PwQI_qGXyJPK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(sess.run(tf.transpose(C)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_YPjtwMyyJaL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "sess = tf.Session()\n",
        "print(sess.run(tf.nn.relu6([-3., 3., 10.])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YM_OMJ_5yJh9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np \n",
        "sess = tf.Session()\n",
        "xvals = np.array([1.,2.,3.])\n",
        "xdata = tf.placeholder(tf.float32)\n",
        "mconst = tf.constant(3.)\n",
        "produ = tf.multiply(xdata, mconst)\n",
        "for i in xvals:\n",
        "  print(sess.run(produ,feed_dict = {xdata:xvals}))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DPaFk6vzh6AS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "my_array = np.array([[1., 3., 5., 7., 9.],\n",
        "                   [-2., 0., 2., 4., 6.],\n",
        "                   [-6., -3., 0., 3., 6.]])\n",
        "x_vals = np.array([my_array, my_array + 1])\n",
        "y = tf.identity(x_vals)\n",
        "x_data = tf.placeholder(tf.float32, shape=(3, 5))\n",
        "print(sess.run(y, feed_dict = {x_data: my_array}))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QqUdbNpMh53a",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "m1 = tf.constant([[1.],[0.],[-1.],[2.],[4.]])\n",
        "m2 = tf.constant([[2.]])\n",
        "a1 = tf.constant([[10.]])\n",
        "prod1 = tf.matmul(x_data, m1)\n",
        "prod2 = tf.matmul(prod1, m2)\n",
        "add1 = tf.add(prod2, a1)\n",
        "for i in x_vals:\n",
        "    print(sess.run(add1, feed_dict={x_data: i}))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FMJqGJNZh5uG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_shape = [1, 4, 4, 1]\n",
        "x_val = np.random.uniform(size=x_shape)\n",
        "x_val"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "850p4XGnh5jZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_val.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BoIMFDUCh5UG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_data = tf.placeholder(tf.float32, shape=x_shape)\n",
        "my_filter = tf.constant(0.25, shape=[2, 2, 1, 1])\n",
        "my_strides = [1, 2, 2, 1]\n",
        "mov_avg_layer= tf.nn.conv2d(x_data, my_filter, my_strides,\n",
        "                            padding='SAME', name='Moving_Avg_Window')\n",
        "sess.run(my_filter)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2Vf_bFzmOgxv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "IRIS\n"
      ]
    },
    {
      "metadata": {
        "id": "kxK4s0SjY6u3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "50b4fef7-27f5-44f3-cd9b-e0ae5e3830d9"
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import datasets\n",
        "import tensorflow as tf\n",
        "sess = tf.Session()\n",
        "iris = datasets.load_iris()\n",
        "iris_2d = np.array([[x[2], x[3]] for x in iris.data])\n",
        "\n",
        "batch_size = 20\n",
        "x1_data = tf.placeholder(shape=[None, 1], dtype=tf.float32)\n",
        "x2_data = tf.placeholder(shape=[None, 1], dtype=tf.float32)\n",
        "y_target = tf.placeholder(shape=[None, 1], dtype=tf.float32)\n",
        "A = tf.Variable(tf.random_normal(shape=[1, 1]))\n",
        "b = tf.Variable(tf.random_normal(shape=[1, 1]))\n",
        "\n",
        "\n",
        "my_add = tf.add(tf.matmul(x2_data, A),b)\n",
        "my_output = tf.subtract(x1_data, my_add)\n",
        "\n",
        "\n",
        "my_mult = tf.matmul(x2_data, A)\n",
        "my_add = tf.add(my_mult, b)\n",
        "my_output = tf.subtract(x1_data, my_add)\n",
        "\n",
        "\n",
        "xentropy = tf.nn.sigmoid_cross_entropy_with_logits(logits=my_output, labels=tf.ones_like(y_target))\n",
        "\n",
        "my_opt = tf.train.GradientDescentOptimizer(0.05)\n",
        "\n",
        "train_step = my_opt.minimize(xentropy)\n",
        "init = tf.initialize_all_variables()\n",
        "sess.run(init)\n",
        "for i in range(1000):\n",
        "    rand_index = np.random.choice(len(iris_2d), size=batch_size)\n",
        "    rand_x = iris_2d[rand_index]\n",
        "    rand_x1 = np.array([[x[0]] for x in rand_x])\n",
        "    rand_x2 = np.array([[x[1]] for x in rand_x])\n",
        "    rand_y = np.array([[y] for y in binary_target[rand_index]])\n",
        "    sess.run(train_step, feed_dict={x1_data: rand_x1, x2_data: rand_x2, y_target: rand_y})\n",
        "    if (i+1)%200==0:\n",
        "        print('Step #' + str(i+1) + ' A = ' + str(sess.run(A)) + ', b = ' + str(sess.run(b)))\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_should_use.py:118: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
            "Instructions for updating:\n",
            "Use `tf.global_variables_initializer` instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-7da9f40ba9b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mrand_x1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrand_x\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mrand_x2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrand_x\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mrand_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbinary_target\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrand_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx1_data\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrand_x1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2_data\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrand_x2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_target\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrand_y\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'binary_target' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "ikFEQKmxY6ps",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "9784574e-43de-4ac7-f974-3f348d096484"
      },
      "cell_type": "code",
      "source": [
        "## FULL CODE - TRAIN TEST - CROSS ENTROPY LOSS \n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.python.framework import ops\n",
        "ops.reset_default_graph()\n",
        "sess = tf.Session()\n",
        "batch_size = 25\n",
        "x_vals = np.concatenate((np.random.normal(-1, 1, 50), np.random.normal(2, 1, 50)))\n",
        "y_vals = np.concatenate((np.repeat(0., 50), np.repeat(1., 50)))\n",
        "x_data = tf.placeholder(shape=[1, None], dtype=tf.float32)\n",
        "y_target = tf.placeholder(shape=[1, None], dtype=tf.float32)\n",
        "train_indices = np.random.choice(len(x_vals), round(len(x_vals)*0.8), replace=False)\n",
        "test_indices = np.array(list(set(range(len(x_vals))) - set(train_indices)))\n",
        "x_vals_train = x_vals[train_indices]\n",
        "x_vals_test = x_vals[test_indices]\n",
        "y_vals_train = y_vals[train_indices]\n",
        "y_vals_test = y_vals[test_indices]\n",
        "A = tf.Variable(tf.random_normal(mean=10, shape=[1]))\n",
        "my_output = tf.add(x_data, A)\n",
        "init = tf.initialize_all_variables()\n",
        "sess.run(init)\n",
        "xentropy = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels = my_output, logits = y_target))\n",
        "my_opt = tf.train.GradientDescentOptimizer(0.05)\n",
        "train_step = my_opt.minimize(xentropy)\n",
        "for i in range(1800):\n",
        "    rand_index = np.random.choice(len(x_vals_train), size=batch_size)\n",
        "    rand_x = [x_vals_train[rand_index]]\n",
        "    rand_y = [y_vals_train[rand_index]]\n",
        "    sess.run(train_step, feed_dict={x_data: rand_x, y_target: rand_y})\n",
        "    if (i+1)%200==0:\n",
        "        print('Step #' + str(i+1) + ' A = ' + str(sess.run(A)))\n",
        "        print('Loss = ' + str(sess.run(xentropy, feed_dict={x_data: rand_x, y_target: rand_y})))\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step #200 A = [15.448706]\n",
            "Loss = -7.8947086\n",
            "Step #400 A = [20.70269]\n",
            "Loss = -7.9752684\n",
            "Step #600 A = [25.978651]\n",
            "Loss = -17.721956\n",
            "Step #800 A = [31.18863]\n",
            "Loss = -22.511877\n",
            "Step #1000 A = [36.396618]\n",
            "Loss = -17.053211\n",
            "Step #1200 A = [41.63061]\n",
            "Loss = -24.95658\n",
            "Step #1400 A = [46.882587]\n",
            "Loss = -28.343739\n",
            "Step #1600 A = [52.110554]\n",
            "Loss = -18.237978\n",
            "Step #1800 A = [57.280518]\n",
            "Loss = -32.047646\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Viu6p5H-Y6ki",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ad7b69d4-337a-49f8-95ef-5a211f824385"
      },
      "cell_type": "code",
      "source": [
        "## MAKING PREDICTION AND CHECKING ACCURACY \n",
        "y_prediction = tf.squeeze(tf.round(tf.nn.sigmoid(tf.add(x_data, A))))\n",
        "correct_prediction = tf.equal(y_prediction, y_target)\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "acc_value_test = sess.run(accuracy, feed_dict={x_data: [x_vals_test], y_target: [y_vals_test]})\n",
        "acc_value_train = sess.run(accuracy, feed_dict={x_data: [x_vals_train], y_target: [y_vals_train]})\n",
        "print('Accuracy on train set: ' + str(acc_value_train))\n",
        "print('Accuracy on test set: ' + str(acc_value_test))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy on train set: 0.525\n",
            "Accuracy on test set: 0.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "68-jMS9JY6UE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tGBotyaDyJov",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9iWln-jeyJvs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9ncGjMPYOp5b",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "MNIST"
      ]
    },
    {
      "metadata": {
        "id": "WvmPuVM0hCKs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        },
        "outputId": "677890e7-3f28-4077-cdb8-e6bef2335c9f"
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot= True)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-18-071d37cb952a>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use urllib or similar directly.\n",
            "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n",
            "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_48VE7r9K0k-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mnist.test.labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K1-XKQOd4nR6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 745
        },
        "outputId": "802afedb-1fe1-42ae-a500-db0981c67bf9"
      },
      "cell_type": "code",
      "source": [
        "learning_rate = 0.5\n",
        "epochs = 10\n",
        "batch_size = 100\n",
        "\n",
        "# declare the training data placeholders\n",
        "# input x - for 28 x 28 pixels = 784\n",
        "x = tf.placeholder(tf.float32, [None, 784])\n",
        "# now declare the output data placeholder - 10 digits\n",
        "y = tf.placeholder(tf.float32, [None, 10])\n",
        "W1 = tf.Variable(tf.random_normal([784, 300], stddev=0.03), name='W1')\n",
        "b1 = tf.Variable(tf.random_normal([300]), name='b1')\n",
        "# and the weights connecting the hidden layer to the output layer\n",
        "W2 = tf.Variable(tf.random_normal([300, 10], stddev=0.03), name='W2')\n",
        "b2 = tf.Variable(tf.random_normal([10]), name='b2')\n",
        "# calculate the output of the hidden layer\n",
        "hidden_out = tf.add(tf.matmul(x, W1), b1)\n",
        "hidden_out = tf.nn.relu(hidden_out)\n",
        "# now calculate the hidden layer output - in this case, let's use a softmax activated\n",
        "# output layer\n",
        "\n",
        "y_ = tf.nn.softmax(tf.add(tf.matmul(hidden_out, W2), b2))\n",
        "y_clipped = tf.clip_by_value(y_, 1e-10, 0.9999999)\n",
        "\n",
        "cross_entropy = -tf.reduce_mean(tf.reduce_sum(y * tf.log(y_clipped)\n",
        "                         + (1 - y) * tf.log(1 - y_clipped), axis=1))\n",
        "\n",
        "optimiser = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cross_entropy)\n",
        "\n",
        "init_op = tf.global_variables_initializer()\n",
        "\n",
        "# define an accuracy assessment operation\n",
        "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "# tf.summary.scalar('accuracy', accuracy)\n",
        "# print(sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels}))\n",
        "merged = tf.summary.merge_all()\n",
        "with tf.Session() as sess:\n",
        "        # initialise the variables\n",
        "        sess.run(init_op)\n",
        "        total_batch = int(len(mnist.train.labels) / batch_size)\n",
        "        for epoch in range(epochs):\n",
        "            avg_cost = 0\n",
        "            for i in range(total_batch):\n",
        "                batch_x, batch_y = mnist.train.next_batch(batch_size=batch_size)\n",
        "                _, c = sess.run([optimiser, cross_entropy], feed_dict={x: batch_x, y: batch_y})\n",
        "                avg_cost += c / total_batch\n",
        "            print(\"Epoch:\", (epoch + 1), \"cost =\", \"{:.3f}\".format(avg_cost))\n",
        "            summary = sess.run(merged, feed_dict={x: mnist.test.images, y: mnist.test.labels})\n",
        "            writer.add_summary(summary, epoch)\n",
        "\n",
        "        print(\"\\nTraining complete!\")\n",
        "        writer.add_graph(sess.graph)\n",
        "        print(sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels}))\n",
        "\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1 cost = 0.718\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-bc8e8dfe9c06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m                 \u001b[0mavg_cost\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtotal_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"cost =\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"{:.3f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg_cost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmerged\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m             \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;31m# Create a fetch handler to take care of the structure of fetches.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m     fetch_handler = _FetchHandler(\n\u001b[0;32m-> 1125\u001b[0;31m         self._graph, fetches, feed_dict_tensor, feed_handles=feed_handles)\n\u001b[0m\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m     \u001b[0;31m# Run request and get response.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, graph, fetches, feeds, feed_handles)\u001b[0m\n\u001b[1;32m    425\u001b[0m     \"\"\"\n\u001b[1;32m    426\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_mapper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_FetchMapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mfor_fetch\u001b[0;34m(fetch)\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfetch\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m       raise TypeError('Fetch argument %r has invalid type %r' % (fetch,\n\u001b[0;32m--> 242\u001b[0;31m                                                                  type(fetch)))\n\u001b[0m\u001b[1;32m    243\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m       \u001b[0;31m# NOTE(touts): This is also the code path for namedtuples.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Fetch argument None has invalid type <class 'NoneType'>"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "8ZhejaI-4nmK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PboxprXw4nsR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hJrgWjXaOsM5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "GOOGLE IMPORT EXPORT "
      ]
    },
    {
      "metadata": {
        "id": "OvvTPNRzGdq_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Install the PyDrive wrapper & import libraries.\n",
        "# This only needs to be done once per notebook.\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once per notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_CwaymVOJcCy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pos = drive.CreateFile({'id': '1GXP__yCsODHRn7nx-wKZb1A_bkk50viT'})\n",
        "pos.GetContentFile('rt-polarity.pos') \n",
        "neg = drive.CreateFile({'id': '1ynUB04DPkxgLFF5pX5a66Gxl2LrwoOl7'})\n",
        "neg.GetContentFile('rt-polarity.neg') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "78Y_LXsqOy65",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**NN on TEXT DATA **"
      ]
    },
    {
      "metadata": {
        "id": "vzn3kE97LebM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "import nltk\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# from nltk.corpus import stopwords as sw\n",
        "\n",
        "# stopwords = sw.words(\"english\") \n",
        "\n",
        "# load the reviews\n",
        "enc='latin-1'\n",
        "positive_reviews = [line.rstrip('\\n') for line in open('rt-polarity.pos', encoding=enc)]\n",
        "negative_reviews = [line.rstrip('\\n') for line in open('rt-polarity.neg', encoding=enc)]\n",
        "\n",
        "print(len(positive_reviews))\n",
        "print(positive_reviews[0])\n",
        "print(type(negative_reviews))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pzg9PC8QLtwV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JYWHeps9KSCN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cPickle\n",
        "from collections import defaultdict\n",
        "import sys, re\n",
        "import pandas as pd\n",
        "\n",
        "def build_data_cv(data_folder, cv=10, clean_string=True):\n",
        "    \"\"\"\n",
        "    Loads data and split into 10 folds.\n",
        "    \"\"\"\n",
        "    revs = []\n",
        "    pos_file = data_folder[0]\n",
        "    neg_file = data_folder[1]\n",
        "    vocab = defaultdict(float)\n",
        "    with open(pos_file, \"rb\") as f:\n",
        "        for line in f:       \n",
        "            rev = []\n",
        "            rev.append(line.strip())\n",
        "            if clean_string:\n",
        "                orig_rev = clean_str(\" \".join(rev))\n",
        "            else:\n",
        "                orig_rev = \" \".join(rev).lower()\n",
        "            words = set(orig_rev.split())\n",
        "            for word in words:\n",
        "                vocab[word] += 1\n",
        "            datum  = {\"y\":1, \n",
        "                      \"text\": orig_rev,                             \n",
        "                      \"num_words\": len(orig_rev.split()),\n",
        "                      \"split\": np.random.randint(0,cv)}\n",
        "            revs.append(datum)\n",
        "    with open(neg_file, \"rb\") as f:\n",
        "        for line in f:       \n",
        "            rev = []\n",
        "            rev.append(line.strip())\n",
        "            if clean_string:\n",
        "                orig_rev = clean_str(\" \".join(rev))\n",
        "            else:\n",
        "                orig_rev = \" \".join(rev).lower()\n",
        "            words = set(orig_rev.split())\n",
        "            for word in words:\n",
        "                vocab[word] += 1\n",
        "            datum  = {\"y\":0, \n",
        "                      \"text\": orig_rev,                             \n",
        "                      \"num_words\": len(orig_rev.split()),\n",
        "                      \"split\": np.random.randint(0,cv)}\n",
        "            revs.append(datum)\n",
        "    return revs, vocab\n",
        "    \n",
        "def get_W(word_vecs, k=300):\n",
        "    \"\"\"\n",
        "    Get word matrix. W[i] is the vector for word indexed by i\n",
        "    \"\"\"\n",
        "    vocab_size = len(word_vecs)\n",
        "    word_idx_map = dict()\n",
        "    W = np.zeros(shape=(vocab_size+1, k), dtype='float32')            \n",
        "    W[0] = np.zeros(k, dtype='float32')\n",
        "    i = 1\n",
        "    for word in word_vecs:\n",
        "        W[i] = word_vecs[word]\n",
        "        word_idx_map[word] = i\n",
        "        i += 1\n",
        "    return W, word_idx_map\n",
        "\n",
        "def load_bin_vec(fname, vocab):\n",
        "    \"\"\"\n",
        "    Loads 300x1 word vecs from Google (Mikolov) word2vec\n",
        "    \"\"\"\n",
        "    word_vecs = {}\n",
        "    with open(fname, \"rb\") as f:\n",
        "        header = f.readline()\n",
        "        vocab_size, layer1_size = map(int, header.split())\n",
        "        binary_len = np.dtype('float32').itemsize * layer1_size\n",
        "        for line in xrange(vocab_size):\n",
        "            word = []\n",
        "            while True:\n",
        "                ch = f.read(1)\n",
        "                if ch == ' ':\n",
        "                    word = ''.join(word)\n",
        "                    break\n",
        "                if ch != '\\n':\n",
        "                    word.append(ch)   \n",
        "            if word in vocab:\n",
        "               word_vecs[word] = np.fromstring(f.read(binary_len), dtype='float32')  \n",
        "            else:\n",
        "                f.read(binary_len)\n",
        "    return word_vecs\n",
        "\n",
        "def add_unknown_words(word_vecs, vocab, min_df=1, k=300):\n",
        "    \"\"\"\n",
        "    For words that occur in at least min_df documents, create a separate word vector.    \n",
        "    0.25 is chosen so the unknown vectors have (approximately) same variance as pre-trained ones\n",
        "    \"\"\"\n",
        "    for word in vocab:\n",
        "        if word not in word_vecs and vocab[word] >= min_df:\n",
        "            word_vecs[word] = np.random.uniform(-0.25,0.25,k)  \n",
        "\n",
        "def clean_str(string, TREC=False):\n",
        "    \"\"\"\n",
        "    Tokenization/string cleaning for all datasets except for SST.\n",
        "    Every dataset is lower cased except for TREC\n",
        "    \"\"\"\n",
        "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)     \n",
        "    string = re.sub(r\"\\'s\", \" \\'s\", string) \n",
        "    string = re.sub(r\"\\'ve\", \" \\'ve\", string) \n",
        "    string = re.sub(r\"n\\'t\", \" n\\'t\", string) \n",
        "    string = re.sub(r\"\\'re\", \" \\'re\", string) \n",
        "    string = re.sub(r\"\\'d\", \" \\'d\", string) \n",
        "    string = re.sub(r\"\\'ll\", \" \\'ll\", string) \n",
        "    string = re.sub(r\",\", \" , \", string) \n",
        "    string = re.sub(r\"!\", \" ! \", string) \n",
        "    string = re.sub(r\"\\(\", \" \\( \", string) \n",
        "    string = re.sub(r\"\\)\", \" \\) \", string) \n",
        "    string = re.sub(r\"\\?\", \" \\? \", string) \n",
        "    string = re.sub(r\"\\s{2,}\", \" \", string)    \n",
        "    return string.strip() if TREC else string.strip().lower()\n",
        "\n",
        "def clean_str_sst(string):\n",
        "    \"\"\"\n",
        "    Tokenization/string cleaning for the SST dataset\n",
        "    \"\"\"\n",
        "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)   \n",
        "    string = re.sub(r\"\\s{2,}\", \" \", string)    \n",
        "    return string.strip().lower()\n",
        "\n",
        "if __name__==\"__main__\":    \n",
        "    w2v_file = sys.argv[1]     \n",
        "    data_folder = [\"rt-polarity.pos\",\"rt-polarity.neg\"]    \n",
        "    print (\"loading data...\",        \n",
        "    revs, vocab = build_data_cv(data_folder, cv=10, clean_string=True))\n",
        "    max_l = np.max(pd.DataFrame(revs)[\"num_words\"])\n",
        "    print (\"data loaded!\")\n",
        "    print (\"number of sentences: \" + str(len(revs)))\n",
        "    print (\"vocab size: \" + str(len(vocab))\n",
        "#     print (\"max sentence length: \" + str(max_l))\n",
        "#     print (\"loading word2vec vectors...\", w2v = load_bin_vec(w2v_file, vocab)\n",
        "#     print (\"word2vec loaded!\")\n",
        "#     print (\"num words already in word2vec: \" + str(len(w2v)))\n",
        "           \n",
        "    add_unknown_words(w2v, vocab)\n",
        "    W, word_idx_map = get_W(w2v)\n",
        "    rand_vecs = {}\n",
        "    add_unknown_words(rand_vecs, vocab)\n",
        "    W2, _ = get_W(rand_vecs)\n",
        "    cPickle.dump([revs, W, W2, word_idx_map, vocab], open(\"mr.p\", \"wb\"))\n",
        "    print (\"dataset created!\")\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SU-DejrpMve4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}